{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33f3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import pickle\n",
    "import cv2\n",
    "from tensorflow.python.client import device_lib\n",
    "import math\n",
    "import pandas as pd\n",
    "device_lib.list_local_devices()\n",
    "import shutil\n",
    "import pathlib\n",
    "import random\n",
    "import sys\n",
    "import colorsys\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecc42cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######creat training data\n",
    "with open(r'D:\\AI in NTU\\PINN-multi-body\\悬挂式空铁训练数据\\Acceleration.txt', 'r', encoding='utf-8') as f:\n",
    "     data = f.readlines()\n",
    "Acceleration =[]  \n",
    "for i in range(len(data)):\n",
    "    Acceleration.append(data[i].strip('\\n').split(\"\\t\"))\n",
    "Time_data =  np.array(Acceleration,dtype=np.float32)[:,0:1]\n",
    "print(Time_data.shape)\n",
    "Acceleration = np.array(Acceleration,dtype=np.float32)[:,1:]\n",
    "print(Acceleration.shape)\n",
    "with open(r'D:\\AI in NTU\\PINN-multi-body\\悬挂式空铁训练数据\\Displacement.txt', 'r', encoding='utf-8') as f:\n",
    "     data = f.readlines()\n",
    "Displacement =[]    \n",
    "for i in range(len(data)):\n",
    "    Displacement.append(data[i].strip('\\n').split(\"\\t\"))\n",
    "Displacement = np.array(Displacement,dtype=np.float32)[:,1:] \n",
    "print(Displacement.shape)\n",
    "with open(r'D:\\AI in NTU\\PINN-multi-body\\悬挂式空铁训练数据\\Velocity.txt', 'r', encoding='utf-8') as f:\n",
    "     data = f.readlines()\n",
    "Velocity =[]    \n",
    "for i in range(len(data)):\n",
    "    Velocity.append(data[i].strip('\\n').split(\"\\t\"))\n",
    "Velocity = np.array(Velocity,dtype=np.float32)[:,1:] \n",
    "print(Velocity.shape)\n",
    "with open(r'D:\\AI in NTU\\PINN-multi-body\\悬挂式空铁训练数据\\Irregularity.txt', 'r', encoding='utf-8') as f:\n",
    "     data = f.readlines()\n",
    "Irregularity =[]    \n",
    "for i in range(len(data)):\n",
    "    Irregularity.append(data[i].strip('\\n').split(\"\\t\"))\n",
    "Irregularity = np.array(Irregularity,dtype=np.float32)[:,1:]\n",
    "print(Irregularity.shape)\n",
    "with open(r'D:\\AI in NTU\\PINN-multi-body\\悬挂式空铁训练数据\\Mass_vehicle.txt', 'r', encoding='utf-8') as f:\n",
    "     data = f.readlines()\n",
    "M_matrix =[]    \n",
    "for i in range(len(data)):\n",
    "    M_matrix.append(data[i].strip('\\n').split(\"\\t\"))\n",
    "M_matrix= np.array(M_matrix,dtype=np.float32) \n",
    "print(M_matrix.shape)\n",
    "with open(r'D:\\AI in NTU\\PINN-multi-body\\悬挂式空铁训练数据\\Damping_vehicle.txt', 'r', encoding='utf-8') as f:\n",
    "     data = f.readlines()\n",
    "C_matrix =[]    \n",
    "for i in range(len(data)):\n",
    "    C_matrix.append(data[i].strip('\\n').split(\"\\t\"))\n",
    "C_matrix= np.array(C_matrix,dtype=np.float32) \n",
    "print(C_matrix.shape)\n",
    "with open(r'D:\\AI in NTU\\PINN-multi-body\\悬挂式空铁训练数据\\Stiffness_vehicle.txt', 'r', encoding='utf-8') as f:\n",
    "     data = f.readlines()\n",
    "K_matrix =[]    \n",
    "for i in range(len(data)):\n",
    "    K_matrix.append(data[i].strip('\\n').split(\"\\t\"))\n",
    "K_matrix= np.array(K_matrix,dtype=np.float32) \n",
    "print(K_matrix.shape)\n",
    "\n",
    "Force =[]  \n",
    "def Force_compensation(num):\n",
    "    tem = Acceleration[num]@M_matrix + Displacement[num]@K_matrix + Velocity[num]@C_matrix\n",
    "    return tem\n",
    "for i in range(len(Displacement)):\n",
    "    Force.append(Force_compensation(i))\n",
    "Force  = np.array(Force ,dtype=np.float32)\n",
    "print(Force.shape) \n",
    "\n",
    "s1  = 5.95/2\n",
    "Hcf = 1.02\n",
    "Hsf = 0.80\n",
    "Ksp = 5e3\n",
    "lo  = math.sqrt(math.pow(0.430,2)+math.pow(0.050,2))\n",
    "LocA = np.array([[-0.15],[0.37]])\n",
    "LocB = np.array([[0.15],[0.37]])\n",
    "LocC = np.array([[-0.20],[-1.02]])\n",
    "LocD = np.array([[0.20],[-1.02]])\n",
    "def FourForce(num):\n",
    "    Tu1 = np.array([[math.cos(Displacement[num][12]),-math.sin(Displacement[num][12])],\n",
    "                    [math.sin(Displacement[num][12]),math.cos(Displacement[num][12])]])\n",
    "    Tu2 =  np.array([[math.cos(Displacement[num][22]),-math.sin(Displacement[num][22])],\n",
    "                     [math.sin(Displacement[num][22]),math.cos(Displacement[num][22])]])\n",
    "    Tp1 = np.array( [[math.cos(Displacement[num][2]),-math.sin(Displacement[num][2])],\n",
    "                     [math.sin(Displacement[num][2]),math.cos(Displacement[num][2])]])\n",
    "    Tp2 =  np.array([[math.cos(Displacement[num][2]),-math.sin(Displacement[num][2])],\n",
    "                     [math.sin(Displacement[num][2]),math.cos(Displacement[num][2])]])\n",
    "\n",
    "    Ru1A = np.array(Tu1@LocA+[[Displacement[num][10]],[(Displacement[num][11]-Hsf)]])\n",
    "    Ru1B = np.array(Tu1@LocB+[[Displacement[num][10]],[(Displacement[num][11]-Hsf)]])\n",
    "    Ru2A = np.array(Tu2@LocA+[[Displacement[num][20]],[(Displacement[num][21]-Hsf)]])\n",
    "    Ru2B = np.array(Tu2@LocB+[[Displacement[num][20]],[(Displacement[num][21]-Hsf)]])\n",
    "\n",
    "    Rp1C = np.array(Tp1@LocC+[[(Displacement[num][0]+s1*Displacement[num][4])],[(Displacement[num][1]-s1*Displacement[num][3]+Hcf)]])\n",
    "    Rp1D = np.array(Tp1@LocD+[[(Displacement[num][0]+s1*Displacement[num][4])],[(Displacement[num][1]-s1*Displacement[num][3]+Hcf)]])\n",
    "    Rp2C = np.array(Tp2@LocC+[[(Displacement[num][0]-s1*Displacement[num][4])],[(Displacement[num][1]+s1*Displacement[num][3]+Hcf)]])\n",
    "    Rp2D = np.array(Tp2@LocD+[[(Displacement[num][0]-s1*Displacement[num][4])],[(Displacement[num][1]+s1*Displacement[num][3]+Hcf)]])\n",
    "    \n",
    "    LAC1 = math.sqrt(math.pow((Rp1C[0]-Ru1A[0]),2)+math.pow((Rp1C[1]-Ru1A[1]),2))\n",
    "    LBD1 = math.sqrt(math.pow((Rp1D[0]-Ru1B[0]),2)+math.pow((Rp1D[0]-Ru1B[0]),2))\n",
    "    LAC2 = math.sqrt(math.pow((Rp2C[0]-Ru2A[0]),2)+math.pow((Rp2C[1]-Ru2A[1]),2))\n",
    "    LBD1 = math.sqrt(math.pow((Rp2D[0]-Ru2B[0]),2)+math.pow((Rp2D[0]-Ru2B[0]),2))\n",
    "    Force =[Ksp*( LAC1-lo),Ksp*(LBD1-lo),Ksp*(LAC2-lo),Ksp*(LBD1-lo)]\n",
    "    return Force\n",
    "\n",
    "Force_Four =[]\n",
    "for i in range(len(Displacement)):\n",
    "    Force_Four.append(FourForce(i))\n",
    "Force_Four  = np.array(Force_Four,dtype=np.float32)\n",
    "print(Force_Four.shape)\n",
    "\n",
    "###划分100长度数据\n",
    "train_Acceleration = Acceleration\n",
    "train_Velocity = Velocity\n",
    "train_Displacement = Displacement\n",
    "train_Irregularity = Irregularity\n",
    "train_Time_data = Time_data\n",
    "train_F = Force\n",
    "train_F4 = Force_Four\n",
    "\n",
    "interval = 100\n",
    "train_A_data=[]\n",
    "train_V_data=[]\n",
    "train_U_data=[]\n",
    "train_Irr_data=[]\n",
    "train_t_data=[]\n",
    "train_F_data=[]\n",
    "train_F4_data=[]\n",
    "for i in range(int((len(train_Acceleration)-100)/interval)):\n",
    "    train_A_data.append(train_Acceleration[i*interval:i*interval+100,:])\n",
    "    train_V_data.append(train_Velocity[i*interval:i*interval+100,:])\n",
    "    train_U_data.append(train_Displacement[i*interval:i*interval+100,:])\n",
    "    train_Irr_data.append(train_Irregularity[i*interval:i*interval+100,:])\n",
    "    train_t_data.append(train_Time_data[i*interval:i*interval+100,:])\n",
    "    train_F_data.append(train_F[i*interval:i*interval+100,:])\n",
    "    train_F4_data.append(train_F4[i*interval:i*interval+100,:])\n",
    "train_A_data=np.array(train_A_data)\n",
    "train_V_data=np.array(train_V_data)\n",
    "train_U_data=np.array(train_U_data)\n",
    "train_Irr_data=np.array(train_Irr_data)\n",
    "train_t_data=np.array(train_t_data)\n",
    "train_F_data=np.array(train_F_data)\n",
    "train_F4_data=np.array(train_F4_data)\n",
    "print(train_A_data.shape)\n",
    "print(train_V_data.shape)\n",
    "print(train_U_data.shape)\n",
    "print(train_Irr_data.shape)\n",
    "print(train_t_data.shape)\n",
    "print(train_F_data.shape)\n",
    "print(train_F4_data.shape)\n",
    "\n",
    "listrandom = [i for i in range(len(train_A_data))]\n",
    "np.random.shuffle(listrandom)\n",
    "\n",
    "train_A_data=train_A_data[listrandom]\n",
    "train_V_data=train_V_data[listrandom]\n",
    "train_U_data=train_U_data[listrandom]\n",
    "train_Irr_data=train_Irr_data[listrandom]\n",
    "train_t_data=train_t_data[listrandom]\n",
    "train_F_data=train_F_data[listrandom]\n",
    "train_F4_data=train_F4_data[listrandom]\n",
    "\n",
    "Ishape = train_Irr_data.shape\n",
    "Ashape = train_A_data.shape\n",
    "Vshape = train_V_data.shape\n",
    "Ushape = train_U_data.shape\n",
    "Fshape = train_F_data.shape\n",
    "F4shape = train_F4_data.shape\n",
    "scaler_I = StandardScaler()\n",
    "scaled_train_Irr_data = scaler_I.fit_transform(train_Irr_data.reshape(-1,1)).reshape(Ishape)\n",
    "scaler_A = StandardScaler()\n",
    "scaler_V = StandardScaler()\n",
    "scaler_U = StandardScaler()\n",
    "scaled_train_A_data = scaler_A.fit_transform(train_A_data.reshape(-1,1)).reshape(Ashape)\n",
    "scaled_train_V_data = scaler_V.fit_transform(train_V_data.reshape(-1,1)).reshape(Vshape)\n",
    "scaled_train_U_data = scaler_U.fit_transform(train_U_data.reshape(-1,1)).reshape(Ushape)\n",
    "scaler_F = StandardScaler()\n",
    "scaled_train_F_data = scaler_F.fit_transform(train_F_data.reshape(-1,1)).reshape(Fshape)\n",
    "scaler_F4 = StandardScaler()\n",
    "scaled_train_F4_data = scaler_F4.fit_transform(train_F4_data.reshape(-1,1)).reshape(F4shape)\n",
    "\n",
    "train_Real_Y_data = np.array([scaled_train_A_data,scaled_train_V_data,scaled_train_U_data])\n",
    "print(train_Real_Y_data.shape)\n",
    "mean_A,std_A = scaler_A.mean_[0], scaler_A.scale_[0]\n",
    "mean_V,std_V = scaler_V.mean_[0], scaler_V.scale_[0]\n",
    "mean_U,std_U = scaler_U.mean_[0], scaler_U.scale_[0]\n",
    "mean_F,std_F = scaler_F.mean_[0], scaler_F.scale_[0]\n",
    "mean_F4,std_F4 = scaler_F4.mean_[0], scaler_F4.scale_[0]\n",
    "\n",
    "Ishape = train_Irr_data.shape\n",
    "Ashape = train_A_data.shape\n",
    "Vshape = train_V_data.shape\n",
    "Ushape = train_U_data.shape\n",
    "Fshape = train_F_data.shape\n",
    "F4shape = train_F4_data.shape\n",
    "scaler_I = StandardScaler()\n",
    "scaled_train_Irr_data = scaler_I.fit_transform(train_Irr_data.reshape(-1,1)).reshape(Ishape)\n",
    "scaler_A = StandardScaler()\n",
    "scaler_V = StandardScaler()\n",
    "scaler_U = StandardScaler()\n",
    "scaled_train_A_data = scaler_A.fit_transform(train_A_data.reshape(-1,1)).reshape(Ashape)\n",
    "scaled_train_V_data = scaler_V.fit_transform(train_V_data.reshape(-1,1)).reshape(Vshape)\n",
    "scaled_train_U_data = scaler_U.fit_transform(train_U_data.reshape(-1,1)).reshape(Ushape)\n",
    "scaler_F = StandardScaler()\n",
    "scaled_train_F_data = scaler_F.fit_transform(train_F_data.reshape(-1,1)).reshape(Fshape)\n",
    "scaler_F4 = StandardScaler()\n",
    "scaled_train_F4_data = scaler_F4.fit_transform(train_F4_data.reshape(-1,1)).reshape(F4shape)\n",
    "\n",
    "train_Real_Y_data = np.array([scaled_train_A_data,scaled_train_V_data,scaled_train_U_data])\n",
    "print(train_Real_Y_data.shape)\n",
    "mean_A,std_A = scaler_A.mean_[0], scaler_A.scale_[0]\n",
    "mean_V,std_V = scaler_V.mean_[0], scaler_V.scale_[0]\n",
    "mean_U,std_U = scaler_U.mean_[0], scaler_U.scale_[0]\n",
    "mean_F,std_F = scaler_F.mean_[0], scaler_F.scale_[0]\n",
    "mean_F4,std_F4 = scaler_F4.mean_[0], scaler_F4.scale_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940dde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V,\n",
    "                                 dropout_rate=0.1,\n",
    "                                 training=True,\n",
    "                                 scope=\"scaled_dot_product_attention\"):\n",
    "    '''\n",
    "    Q: Packed queries. 3d tensor. [N, T_q, d_k].\n",
    "    K: Packed keys. 3d tensor. [N, T_k, d_k].\n",
    "    V: Packed values. 3d tensor. [N, T_k, d_v].\n",
    "    key_masks: A 2d tensor with shape of [N, key_seqlen]\n",
    "    causality: If True, applies masking for future blinding\n",
    "    dropout_rate: A floating point number of [0, 1].\n",
    "    training: boolean for controlling droput\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "    '''\n",
    "    d_k = Q.get_shape().as_list()[-1]\n",
    "\n",
    "    # dot product\n",
    "    outputs = tf.matmul(Q, tf.transpose(K, [0, 2, 1]))  # (N, T_q, T_k)\n",
    "\n",
    "    # scale\n",
    "    outputs /= d_k ** 0.5\n",
    "\n",
    "    # softmax\n",
    "    outputs = tf.nn.softmax(outputs)\n",
    "\n",
    "    # dropout\n",
    "    outputs = tf.keras.layers.Dropout(dropout_rate)(outputs)\n",
    "\n",
    "    # weighted sum (context vectors)\n",
    "    outputs = tf.matmul(outputs, V)  # (N, T_q, d_v)\n",
    "\n",
    "    return outputs\n",
    "    \n",
    "def multihead_attention(queries, keys, values,\n",
    "                        num_heads=8, \n",
    "                        dropout_rate=0.1,\n",
    "                        training=True,\n",
    "                        scope=\"multihead_attention\"):\n",
    "    '''\n",
    "    queries: A 3d tensor with shape of [N, T_q, d_model].\n",
    "    keys: A 3d tensor with shape of [N, T_k, d_model].\n",
    "    values: A 3d tensor with shape of [N, T_k, d_model].\n",
    "    key_masks: A 2d tensor with shape of [N, key_seqlen]\n",
    "    num_heads: An int. Number of heads.\n",
    "    dropout_rate: A floating point number.\n",
    "    training: Boolean. Controller of mechanism for dropout.\n",
    "    causality: Boolean. If true, units that reference the future are masked.\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "        \n",
    "    Returns\n",
    "      A 3d tensor with shape of (N, T_q, C)  \n",
    "    '''\n",
    "    d_model = queries.get_shape().as_list()[-1]\n",
    "    # Linear projections\n",
    "    Q = tf.keras.layers.Dense(d_model)(queries) # (N, T_q, d_model) \n",
    "    K = tf.keras.layers.Dense(d_model)(keys) # (N, T_k, d_model)\n",
    "    V = tf.keras.layers.Dense(d_model)(values)# (N, T_v, d_model)\n",
    "    \n",
    "    # Split and concat\n",
    "    Q_ = tf.concat(tf.split(Q, num_heads, axis=2), axis=0) # (h*N, T_q, d_model/h)\n",
    "    K_ = tf.concat(tf.split(K, num_heads, axis=2), axis=0) # (h*N, T_k, d_model/h)\n",
    "    V_ = tf.concat(tf.split(V, num_heads, axis=2), axis=0) # (h*N, T_v, d_model/h)\n",
    "\n",
    "    # Attention\n",
    "    outputs =  scaled_dot_product_attention(Q_, K_, V_, dropout_rate, training)\n",
    "\n",
    "    # Restore shape\n",
    "    outputs = tf.concat(tf.split(outputs, num_heads, axis=0), axis=2 ) # (N, T_q, d_model)\n",
    "\n",
    "    # Residual connection\n",
    "    outputs = tf.keras.layers.add([queries,outputs])\n",
    "\n",
    "    # Normalize\n",
    "    outputs = tf.keras.layers.LayerNormalization()(outputs)\n",
    " \n",
    "    return outputs\n",
    "\n",
    "def ff(inputs, num_units, scope=\"positionwise_feedforward\"):\n",
    "    '''\n",
    "    inputs: A 3d tensor with shape of [N, T, C].\n",
    "    num_units: A list of two integers.\n",
    "    scope: Optional scope for `variable_scope`.\n",
    "    Returns:\n",
    "      A 3d tensor with the same shape and dtype as inputs\n",
    "    '''\n",
    "    # Inner layer\n",
    "    outputs = tf.keras.layers.Dense(num_units[0], activation=tf.nn.relu)(inputs) \n",
    "\n",
    "    # Outer layer\n",
    "    outputs = tf.keras.layers.Dense(num_units[1])(outputs)\n",
    "\n",
    "    # Residual connection\n",
    "    outputs = tf.keras.layers.concatenate([outputs, inputs],axis=-1)\n",
    "\n",
    "    # Normalize\n",
    "    outputs = tf.keras.layers.LayerNormalization()(outputs)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000 ** (2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000 ** (2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'dtype': self.dtype,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b206d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SeATransConv_PINN():\n",
    "    #####part1\n",
    "    Irregularity_input = tf.keras.layers.Input(shape=(100,16)) #后4列为轨道不平顺值\n",
    "    Irregularity_F = tf.keras.layers.Input(shape=(100,25)) #F值\n",
    "    Irregularity_F4 = tf.keras.layers.Input(shape=(100,4)) #F4值\n",
    "    Irregularity_newF = tf.concat([Irregularity_F,Irregularity_F4],-1)\n",
    "    encoder_in = tf.keras.layers.Dense(64)(Irregularity_input) #(None,100,64)\n",
    "    decoder_in = tf.keras.layers.Dense(64)(Irregularity_newF) #(None,100,64)\n",
    "   \n",
    "    positional_encoding = PositionalEncoding(max_steps=100, max_dims=64)\n",
    "    \n",
    "    encoder_in = positional_encoding(encoder_in)\n",
    "    decoder_in = positional_encoding(decoder_in)\n",
    "    \n",
    "    layer1 = multihead_attention(encoder_in,encoder_in,decoder_in)\n",
    "    layer2 = ff(layer1, [128,128])\n",
    "    \n",
    "    layer3 = multihead_attention(encoder_in, decoder_in, decoder_in)\n",
    "    layer4 = multihead_attention(layer3, layer2, decoder_in)\n",
    "    \n",
    "    layer5 = ff(layer4, [128,128])\n",
    "    layer6 = tf.keras.layers.Dense(64, activation=tf.nn.leaky_relu)(layer5)\n",
    "    LayerNorm1 = tf.keras.layers.LayerNormalization(axis=-1)(layer6)\n",
    "    Dense3 =  tf.keras.layers.Dense(128,activation=tf.nn.leaky_relu)(LayerNorm1)\n",
    "    layer_A = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense3)\n",
    "    LayerNormA = tf.keras.layers.LayerNormalization(axis=-1)(layer_A)\n",
    "    layer_A = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(LayerNormA)\n",
    "    A = tf.keras.layers.Dense(25)(layer_A)\n",
    "    layer_V = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense3)\n",
    "    LayerNormV = tf.keras.layers.LayerNormalization(axis=-1)(layer_V)\n",
    "    layer_V = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(LayerNormV)\n",
    "    V = tf.keras.layers.Dense(25)(layer_V)\n",
    "    layer_U = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(Dense3)\n",
    "    LayerNormU = tf.keras.layers.LayerNormalization(axis=-1)(layer_U)\n",
    "    layer_U = tf.keras.layers.Dense(64,activation=tf.nn.leaky_relu)(LayerNormU)\n",
    "    U = tf.keras.layers.Dense(25)(layer_U)\n",
    "    #####part2\n",
    "    Conv_input = tf.keras.layers.concatenate([encoder_in[...,tf.newaxis],decoder_in[...,tf.newaxis]],axis=-1)\n",
    "    Conv_1 = tf.keras.layers.Conv2D(25,kernel_size = 2,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_input)\n",
    "    Conv_1T = tf.transpose(Conv_1,[0,1,3,2])\n",
    "    Conv_2 = tf.keras.layers.Conv2D(64,kernel_size = 2,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_1T)\n",
    "    Conv_3 = tf.keras.layers.Conv2D(64,kernel_size = 2,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_2)\n",
    "    Conv_4 = tf.keras.layers.concatenate([Conv_2,Conv_3],axis=-1)\n",
    "    Conv_5 = tf.keras.layers.Conv2D(64,kernel_size = 3,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_4)\n",
    "    Conv_6 = tf.keras.layers.Conv2D(64,kernel_size = 3,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(tf.concat([Conv_3,Conv_5],axis=-1))\n",
    "    Conv_7 = tf.keras.layers.concatenate([Conv_5,Conv_6],axis=-1)\n",
    "    Conv_A = tf.keras.layers.Conv2D(64,kernel_size = 2,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_7)\n",
    "    C_A = tf.squeeze(tf.keras.layers.Conv2D(1, kernel_size = 2,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_A))\n",
    "    Conv_V = tf.keras.layers.Conv2D(64,kernel_size = 2,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_7)\n",
    "    C_V = tf.squeeze(tf.keras.layers.Conv2D(1, kernel_size = 2,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_V))\n",
    "    Conv_U = tf.keras.layers.Conv2D(64,kernel_size = 2,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_7)\n",
    "    C_U = tf.squeeze(tf.keras.layers.Conv2D(1, kernel_size = 2,strides = 1, padding=\"same\", activation=tf.nn.leaky_relu)(Conv_U))\n",
    "    #####adaptive weights\n",
    "    weight_1 = tf.keras.layers.GlobalMaxPooling1D()(Dense3)\n",
    "    weight_1 = tf.keras.layers.Dense(2500)(weight_1)\n",
    "    weight_1 = tf.keras.layers.Reshape((100,25))(weight_1)\n",
    "    weight_2 = tf.keras.layers.GlobalMaxPooling2D()(Conv_7)\n",
    "    weight_2 = tf.keras.layers.Dense(2500)(weight_2)\n",
    "    weight_2 = tf.keras.layers.Reshape((100,25))(weight_2)\n",
    "    ####合成\n",
    "    Final_A = weight_1*A + weight_2 * C_A\n",
    "    Final_V = weight_1*V + weight_2 * C_V\n",
    "    Final_U = weight_1*U + weight_2 * C_U\n",
    "    #固化模型 \n",
    "    model=tf.keras.models.Model(inputs=[Irregularity_input,Irregularity_F,Irregularity_F4],outputs=[Final_A,Final_V,Final_U])   \n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeae3c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SeATransConv_PINN()\n",
    "#model.load_weights('./SAT_PINN.h5')\n",
    "History=[]\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "@tf.function\n",
    "def FourForce_loss(Displacement):\n",
    "    with tf.device('/cpu:0'):\n",
    "        Tu11 = tf.concat((tf.cos(Displacement[...,12])[...,tf.newaxis],  tf.sin(Displacement[...,12])[...,tf.newaxis]),-1)[...,tf.newaxis]\n",
    "        Tu12 = tf.concat((-tf.sin(Displacement[...,12])[...,tf.newaxis], tf.cos(Displacement[...,12])[...,tf.newaxis]),-1)[...,tf.newaxis]\n",
    "        Tu1 = tf.concat((Tu11,Tu12),-1)\n",
    "\n",
    "        Tu21 = tf.concat((tf.cos(Displacement[...,22])[...,tf.newaxis],  tf.sin(Displacement[...,22])[...,tf.newaxis]),-1)[...,tf.newaxis]\n",
    "        Tu22 = tf.concat((-tf.sin(Displacement[...,22])[...,tf.newaxis], tf.cos(Displacement[...,22])[...,tf.newaxis]),-1)[...,tf.newaxis]\n",
    "        Tu2 = tf.concat((Tu21,Tu22),-1)\n",
    "\n",
    "        Tp11 = tf.concat((tf.cos(Displacement[...,2])[...,tf.newaxis],  tf.sin(Displacement[...,2])[...,tf.newaxis]),-1)[...,tf.newaxis]\n",
    "        Tp12 = tf.concat((-tf.sin(Displacement[...,2])[...,tf.newaxis], tf.cos(Displacement[...,2])[...,tf.newaxis]),-1)[...,tf.newaxis]\n",
    "        Tp1 = tf.concat((Tp11,Tp12),-1)\n",
    "\n",
    "        Tp21 = tf.concat((tf.cos(Displacement[...,2])[...,tf.newaxis], tf.sin(Displacement[...,2])[...,tf.newaxis]),-1)[...,tf.newaxis]\n",
    "        Tp22 = tf.concat((-tf.sin(Displacement[...,2])[...,tf.newaxis], tf.cos(Displacement[...,2])[...,tf.newaxis]),-1)[...,tf.newaxis]\n",
    "        Tp2 = tf.concat((Tp21,Tp22),-1)\n",
    "\n",
    "        Ru1A = tf.squeeze(Tu1@LocA)+tf.concat((Displacement[...,10][...,tf.newaxis],(Displacement[...,11]-Hsf)[...,tf.newaxis]),2)\n",
    "        Ru1B = tf.squeeze(Tu1@LocB)+tf.concat((Displacement[...,10][...,tf.newaxis],(Displacement[...,11]-Hsf)[...,tf.newaxis]),2)\n",
    "        Ru2A = tf.squeeze(Tu2@LocA)+tf.concat((Displacement[...,20][...,tf.newaxis],(Displacement[...,21]-Hsf)[...,tf.newaxis]),2)\n",
    "        Ru2B = tf.squeeze(Tu2@LocB)+tf.concat((Displacement[...,20][...,tf.newaxis],(Displacement[...,21]-Hsf)[...,tf.newaxis]),2)\n",
    "        \n",
    "        Rp1C = tf.squeeze(Tp1@LocC)+ tf.concat(((Displacement[...,0]+s1*Displacement[...,4])[...,tf.newaxis],\n",
    "                                                (Displacement[...,1]-s1*Displacement[...,3]+Hcf)[...,tf.newaxis]),2)\n",
    "        Rp1D = tf.squeeze(Tp1@LocD)+ tf.concat(((Displacement[...,0]+s1*Displacement[...,4])[...,tf.newaxis],\n",
    "                                                (Displacement[...,1]-s1*Displacement[...,3]+Hcf)[...,tf.newaxis]),2)\n",
    "        Rp2C = tf.squeeze(Tp2@LocC)+ tf.concat(((Displacement[...,0]-s1*Displacement[...,4])[...,tf.newaxis],\n",
    "                                                (Displacement[...,1]+s1*Displacement[...,3]+Hcf)[...,tf.newaxis]),2)\n",
    "        Rp2D = tf.squeeze(Tp2@LocD)+ tf.concat(((Displacement[...,0]-s1*Displacement[...,4])[...,tf.newaxis],\n",
    "                                                (Displacement[...,1]+s1*Displacement[...,3]+Hcf)[...,tf.newaxis]),2)\n",
    "\n",
    "        LAC1 = tf.sqrt(tf.pow((Rp1C[...,0]-Ru1A[...,0]),2)+tf.pow((Rp1C[...,1]-Ru1A[...,1]),2))\n",
    "        LBD1 = tf.sqrt(tf.pow((Rp1D[...,0]-Ru1B[...,0]),2)+tf.pow((Rp1D[...,0]-Ru1B[...,0]),2))\n",
    "        LAC2 = tf.sqrt(tf.pow((Rp2C[...,0]-Ru2A[...,0]),2)+tf.pow((Rp2C[...,1]-Ru2A[...,1]),2))\n",
    "        LBD1 = tf.sqrt(tf.pow((Rp2D[...,0]-Ru2B[...,0]),2)+tf.pow((Rp2D[...,0]-Ru2B[...,0]),2))\n",
    "\n",
    "        Force_predcited = tf.concat((Ksp*(LAC1-lo)[...,tf.newaxis],Ksp*(LBD1-lo)[...,tf.newaxis],\n",
    "                                     Ksp*(LAC2-lo)[...,tf.newaxis],Ksp*(LBD1-lo)[...,tf.newaxis]),-1)\n",
    "                                 \n",
    "    return Force_predcited \n",
    "\n",
    "@tf.function\n",
    "def train_step_PINN(x,y1,y2,y3,F,F4,weight1,weight2):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_A,predicted_V,predicted_U = model([x,F,F4])\n",
    "        loss1 = tf.reduce_mean(tf.keras.losses.MAE(y1,predicted_A))\n",
    "        loss2 = tf.reduce_mean(tf.keras.losses.MAE(y2,predicted_V))\n",
    "        loss3 = tf.reduce_mean(tf.keras.losses.MAE(y3,predicted_U))\n",
    "        lossd  = loss1+loss2+loss3\n",
    "        ###还原\n",
    "        Inverse_A = (mean_A + predicted_A*std_A)\n",
    "        Inverse_V = (mean_V + predicted_V*std_V)\n",
    "        Inverse_U = (mean_U + predicted_U*std_U)\n",
    "        Inverse_F = (mean_F + F*std_F)\n",
    "        Inverse_F4 = (mean_F4 + F4*std_F4)\n",
    "        lossf1 = tf.reduce_mean(tf.keras.losses.MAE(Inverse_F,(Inverse_A@M_matrix + Inverse_V@C_matrix + Inverse_U@K_matrix)))\n",
    "        Force_predcited = FourForce_loss(Inverse_U)\n",
    "        lossf2 = tf.reduce_mean(tf.keras.losses.MAE(Inverse_F4,Force_predcited))\n",
    "        \n",
    "        loss = weight1 * lossd + lossf1 + weight2 * lossf2\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss,lossf1,lossf2,lossd\n",
    "\n",
    "@tf.function\n",
    "def valid_step_PINN(x,y1,y2,y3,F,F4):\n",
    "    predicted_A,predicted_V,predicted_U =  model([x,F,F4])\n",
    "    loss1 = tf.reduce_mean(tf.keras.losses.MAE(y1,predicted_A))\n",
    "    loss2 = tf.reduce_mean(tf.keras.losses.MAE(y2,predicted_V))\n",
    "    loss3 = tf.reduce_mean(tf.keras.losses.MAE(y3,predicted_U))\n",
    "    lossd  = loss1+loss2+loss3\n",
    "    return lossd\n",
    "\n",
    "#####training_step1######\n",
    "for epoch in range(0,500,1):\n",
    "    for increase in range(4):\n",
    "        Average_loss_f1 = 0\n",
    "        Average_loss_f2 = 0\n",
    "        Average_loss_d = 0\n",
    "        Valid_loss = 0\n",
    "        batch_size=8\n",
    "        num = np.arange(int(len(scaled_train_Irr_data)/batch_size))\n",
    "        #np.random.shuffle(num)\n",
    "        train_index = num[:int(len(num)*0.8)]\n",
    "        weight1 = 1e4\n",
    "        weight2 = 1e2\n",
    "        for i in train_index:\n",
    "            tem_Irr_data = np.array(scaled_train_Irr_data[i*batch_size:(i+1)*batch_size])\n",
    "            tem_A_data = np.array(scaled_train_A_data[i*batch_size:(i+1)*batch_size])\n",
    "            tem_V_data = np.array(scaled_train_V_data[i*batch_size:(i+1)*batch_size])\n",
    "            tem_U_data = np.array(scaled_train_U_data[i*batch_size:(i+1)*batch_size])\n",
    "            tem_F_data = np.array(scaled_train_F_data[i*batch_size:(i+1)*batch_size])\n",
    "            tem_F4_data = np.array(scaled_train_F4_data[i*batch_size:(i+1)*batch_size])\n",
    "            loss,lossf1,lossf2,lossd = train_step_PINN(tem_Irr_data,tem_A_data,tem_V_data,tem_U_data,tem_F_data,tem_F4_data,weight1,weight2)\n",
    "            weight2 = np.array(tf.pow(10,int(tf.experimental.numpy.log10(lossf1/lossf2))+2),dtype=np.float32)\n",
    "            weight1 = np.array(tf.pow(10,int(tf.experimental.numpy.log10((lossf1+weight2*lossf2)/lossd))+ 1),dtype=np.float32)\n",
    "            Average_loss_f1 +=lossf1\n",
    "            Average_loss_f2 +=lossf2\n",
    "            Average_loss_d +=lossd\n",
    "        valid_index = num[int(len(num)*0.8):]\n",
    "        for i in valid_index:\n",
    "            tem_Irr_data = np.array(scaled_train_Irr_data[i*batch_size:(i+1)*batch_size],dtype=np.float32)\n",
    "            tem_A_data = np.array(scaled_train_A_data[i*batch_size:(i+1)*batch_size],dtype=np.float32)\n",
    "            tem_V_data = np.array(scaled_train_V_data[i*batch_size:(i+1)*batch_size],dtype=np.float32)\n",
    "            tem_U_data = np.array(scaled_train_U_data[i*batch_size:(i+1)*batch_size],dtype=np.float32)\n",
    "            tem_F_data = np.array(scaled_train_F_data[i*batch_size:(i+1)*batch_size])\n",
    "            tem_F4_data = np.array(scaled_train_F4_data[i*batch_size:(i+1)*batch_size])\n",
    "            loss_valid = valid_step_PINN(tem_Irr_data,tem_A_data,tem_V_data,tem_U_data,tem_F_data,tem_F4_data)\n",
    "            Valid_loss +=loss_valid\n",
    "        Average_loss_f1 = Average_loss_f1 /len(train_index)\n",
    "        Average_loss_f2 = Average_loss_f2 /len(train_index)\n",
    "        Average_loss_d = Average_loss_d /len(train_index)\n",
    "        Valid_loss     = Valid_loss/len(valid_index)\n",
    "    History.append([Average_loss_f1,Average_loss_f2,Average_loss_d,Valid_loss,weight1,weight2])\n",
    "    print(epoch, 'lossf1:', float(Average_loss_f1),'lossf2:', float(Average_loss_f2), \n",
    "          'lossd:', float(Average_loss_d), 'lvalid_loss:', float(Valid_loss),\n",
    "          'weight1:', float(weight1), 'weight2:', float(weight2))    \n",
    "    model.save_weights('./SAT_PINN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb1d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################test################\n",
    "model = SeATransConv_PINN()\n",
    "model.load_weights('./SAT_PINN.h5')\n",
    "batch_size=8\n",
    "num = np.arange(int(len(scaled_train_Irr_data)/batch_size))\n",
    "\n",
    "predicted_results_A = []\n",
    "predicted_results_V = []\n",
    "predicted_results_U = []\n",
    "real_results_A = []\n",
    "real_results_V = []\n",
    "real_results_U = []\n",
    "\n",
    "valid_index = num[int(len(num)*0.8):]\n",
    "for i in valid_index:\n",
    "    tem_Irr_data = np.array(scaled_train_Irr_data[i*batch_size:(i+1)*batch_size],dtype=np.float32)\n",
    "    tem_A_data = np.array(scaled_train_A_data[i*batch_size:(i+1)*batch_size],dtype=np.float32)\n",
    "    tem_V_data = np.array(scaled_train_V_data[i*batch_size:(i+1)*batch_size],dtype=np.float32)\n",
    "    tem_U_data = np.array(scaled_train_U_data[i*batch_size:(i+1)*batch_size],dtype=np.float32)\n",
    "    tem_F_data = np.array(scaled_train_F_data[i*batch_size:(i+1)*batch_size])\n",
    "    tem_F4_data = np.array(scaled_train_F4_data[i*batch_size:(i+1)*batch_size])\n",
    "    predicted_A,predicted_V,predicted_U =  model([tem_Irr_data,tem_F_data,tem_F4_data])\n",
    "    for j in range(batch_size):\n",
    "        predicted_results_A.extend(np.squeeze(mean_A + predicted_A[j]*std_A))\n",
    "        predicted_results_V.extend(np.squeeze(mean_V + predicted_V[j]*std_V))\n",
    "        predicted_results_U.extend(np.squeeze(mean_U + predicted_U[j]*std_U))\n",
    "        real_results_A.extend(np.squeeze(mean_A +tem_A_data[j]*std_A))\n",
    "        real_results_V.extend(np.squeeze(mean_V +tem_V_data[j]*std_V))\n",
    "        real_results_U.extend(np.squeeze(mean_U +tem_U_data[j]*std_U))\n",
    "\n",
    "predicted_results_A = np.array(predicted_results_A)[:1000]\n",
    "predicted_results_V = np.array(predicted_results_V)[:1000]\n",
    "predicted_results_U = np.array(predicted_results_U)[:1000]\n",
    "real_results_A = np.array(real_results_A)[:1000]\n",
    "real_results_V = np.array(real_results_V)[:1000]\n",
    "real_results_U = np.array(real_results_U)[:1000]     \n",
    "\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(predicted_results_A)\n",
    "plt.show()\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(real_results_A)\n",
    "plt.show()\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(predicted_results_V)\n",
    "plt.show()\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(real_results_V)\n",
    "plt.show()\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(predicted_results_U)\n",
    "plt.show()\n",
    "plt.figure(figsize=(25,5))\n",
    "plt.plot(real_results_U)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
